{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b29efa",
   "metadata": {},
   "source": [
    "# RT-DETR-X: Fine-tuning and Evaluation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Install required dependencies\n",
    "2. Prepare dataset for training\n",
    "3. Fine-tune RT-DETR-X (extra-large variant) on a custom dataset\n",
    "4. Run inference on test images\n",
    "5. Calculate and visualize evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c5bd1",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ultralytics package for RT-DETR\n",
    "!pip install ultralytics\n",
    "!pip install opencv-python matplotlib seaborn\n",
    "\n",
    "# Check CUDA availability \n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3485286",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import RTDETR\n",
    "from IPython.display import display, Image\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c1f90",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Using the same dataset format as our previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5491b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths - customize these for your specific project\n",
    "DATASET_DIR = \"../dataset_split\"  # Change this!\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "# Write the dataset configuration to a YAML file\n",
    "yaml_path = os.path.join(DATASET_DIR, \"data.yaml\")\n",
    "print(f\"Using dataset configuration from: {yaml_path}\")\n",
    "\n",
    "# Check if the dataset exists\n",
    "if not os.path.exists(yaml_path):\n",
    "    print(\"Warning: Dataset configuration not found. Please create or verify the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f453b",
   "metadata": {},
   "source": [
    "## 4. Load and Fine-tune RT-DETR-X Model\n",
    "\n",
    "Now we'll load a pre-trained RT-DETR-X model and fine-tune it on our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72520906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained RT-DETR-X model directly\n",
    "model = RTDETR('rtdetr-x.pt')  # Loading just like a YOLO model\n",
    "\n",
    "# Define training hyperparameters optimized for small dataset (~400 images)\n",
    "hyperparameters = {\n",
    "    'epochs': 100,          # More epochs for small dataset\n",
    "    'batch': 8,             # Smaller batch size due to larger model\n",
    "    'imgsz': 640,           # Image size\n",
    "    'patience': 20,         # Increased patience for early stopping\n",
    "    'device': 0,            # Device to use (0 for first GPU)\n",
    "    'workers': 4,           # Reduced worker threads\n",
    "    'optimizer': 'AdamW',   # Optimizer\n",
    "    'lr0': 0.0005,          # Lower initial learning rate for larger model\n",
    "    'lrf': 0.01,            # Final learning rate factor\n",
    "    'momentum': 0.937,      # SGD momentum\n",
    "    'weight_decay': 0.001,  # Increased weight decay to prevent overfitting\n",
    "    'warmup_epochs': 5.0,   # Longer warmup\n",
    "    'warmup_momentum': 0.8, # Warmup momentum\n",
    "    'warmup_bias_lr': 0.1,  # Warmup bias lr\n",
    "    'box': 7.5,             # Box loss gain\n",
    "    'cls': 0.5,             # Class loss gain\n",
    "    'hsv_h': 0.015,         # Image HSV-Hue augmentation\n",
    "    'hsv_s': 0.7,           # Image HSV-Saturation augmentation\n",
    "    'hsv_v': 0.4,           # Image HSV-Value augmentation\n",
    "    'translate': 0.2,       # Increased translation for better augmentation\n",
    "    'scale': 0.6,           # Increased scale variation\n",
    "    'fliplr': 0.5,          # Image flip left-right probability\n",
    "    'flipud': 0.2,          # Add up-down flipping\n",
    "    'mosaic': 1.0,          # Maximize mosaic augmentation\n",
    "    'mixup': 0.15,          # Add mixup augmentation\n",
    "    'copy_paste': 0.1,      # Add copy-paste augmentation\n",
    "}\n",
    "\n",
    "# Create model results directory\n",
    "results_dir = os.path.join(os.getcwd(), \"rtdetr_x_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    project=results_dir,\n",
    "    name='fine_tuned_model',\n",
    "    exist_ok=True,\n",
    "    **hyperparameters\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Model saved to: {os.path.join(results_dir, 'fine_tuned_model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe514acf",
   "metadata": {},
   "source": [
    "## 5. Model Inference and Evaluation\n",
    "\n",
    "Now, let's evaluate the fine-tuned model on the test set and calculate performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model_path = os.path.join(results_dir, 'fine_tuned_model', 'weights', 'best.pt')\n",
    "model = RTDETR(fine_tuned_model_path)\n",
    "\n",
    "# Run validation on the test set\n",
    "test_results = model.val(\n",
    "    data=yaml_path,\n",
    "    split='test',  # Use the test split\n",
    "    imgsz=640,\n",
    "    batch=8,      # Smaller batch size for evaluation due to model size\n",
    "    verbose=True,\n",
    "    conf=0.25,    # Confidence threshold\n",
    "    iou=0.5,      # IoU threshold\n",
    "    project=results_dir,\n",
    "    name='evaluation',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(\"Test results summary:\")\n",
    "print(f\"mAP50: {test_results.box.map50:.5f}\")\n",
    "print(f\"mAP50-95: {test_results.box.map:.5f}\")\n",
    "print(f\"Precision: {test_results.box.mp:.5f}\")\n",
    "print(f\"Recall: {test_results.box.mr:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ff13a",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis per Class\n",
    "\n",
    "Let's analyze the model performance for each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class-wise metrics from the validation results\n",
    "class_map = test_results.names  # Class index to name mapping\n",
    "\n",
    "# Access class metrics correctly from test_results.box\n",
    "# The DetMetrics object doesn't have a 'metrics' attribute as per the error\n",
    "class_precisions = test_results.box.p  # Class precisions\n",
    "class_recalls = test_results.box.r     # Class recalls\n",
    "ap50_per_class = test_results.box.ap50  # AP50 per class\n",
    "ap_per_class = test_results.box.ap      # AP50-95 per class\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': [class_map[i] for i in range(len(class_map))],\n",
    "    'AP50': ap50_per_class,\n",
    "    'AP50-95': ap_per_class,\n",
    "    'Precision': class_precisions,\n",
    "    'Recall': class_recalls\n",
    "})\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Plot AP50 for each class\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Class', y='AP50', data=metrics_df)\n",
    "plt.title('AP50 for Each Class')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f0fc2",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix\n",
    "\n",
    "The confusion matrix helps us see how well the model differentiates between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7032c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_matrix = test_results.confusion_matrix.matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    conf_matrix / np.sum(conf_matrix, axis=1)[:, None],  # Normalize by row (true classes)\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=list(class_map.values()),\n",
    "    yticklabels=list(class_map.values())\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcc2ee",
   "metadata": {},
   "source": [
    "## 8. Visualizing Detection Results on Test Images\n",
    "\n",
    "Let's visualize some predictions on test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of test images\n",
    "test_images_dir = os.path.join(TEST_DIR, 'images')\n",
    "test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "test_images = [str(img) for img in test_images]\n",
    "\n",
    "# Select random images for visualization\n",
    "if len(test_images) > 0:\n",
    "    sample_images = random.sample(test_images, min(5, len(test_images)))\n",
    "    \n",
    "    for img_path in sample_images:\n",
    "        # Run inference\n",
    "        results = model(img_path, conf=0.25)\n",
    "        \n",
    "        # Display results\n",
    "        for result in results:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
    "            img = result.orig_img\n",
    "            \n",
    "            # Plot detections\n",
    "            for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
    "                x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "                class_id = int(cls.item())\n",
    "                class_name = class_map[class_id]\n",
    "                confidence = conf.item()\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{class_name}: {confidence:.2f}\"\n",
    "                cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f\"Predictions on {os.path.basename(img_path)}\")\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No test images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8d6f6",
   "metadata": {},
   "source": [
    "## 9. Inference Speed Analysis\n",
    "\n",
    "Let's evaluate the inference speed of RT-DETR-X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef879f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure inference time\n",
    "def measure_inference_time(model, image_path, num_runs=50):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(img)\n",
    "    \n",
    "    # Measure inference time\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = model(img)\n",
    "        inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        times.append(inference_time)\n",
    "    \n",
    "    return {\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'min_time': np.min(times),\n",
    "        'max_time': np.max(times),\n",
    "        'times': times\n",
    "    }\n",
    "\n",
    "# Measure inference time if test images are available\n",
    "if len(test_images) > 0:\n",
    "    sample_image = test_images[0]\n",
    "    timing_results = measure_inference_time(model, sample_image)\n",
    "    \n",
    "    print(f\"Inference Speed Analysis for RT-DETR-X:\")\n",
    "    print(f\"Mean inference time: {timing_results['mean_time']:.2f} ms\")\n",
    "    print(f\"Standard deviation: {timing_results['std_time']:.2f} ms\")\n",
    "    print(f\"Min inference time: {timing_results['min_time']:.2f} ms\")\n",
    "    print(f\"Max inference time: {timing_results['max_time']:.2f} ms\")\n",
    "    \n",
    "    # Plot histogram of inference times\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(timing_results['times'], bins=20, alpha=0.7, color='blue')\n",
    "    plt.axvline(timing_results['mean_time'], color='red', linestyle='dashed', linewidth=2, label=f\"Mean: {timing_results['mean_time']:.2f} ms\")\n",
    "    plt.title('RT-DETR-X Inference Time Distribution')\n",
    "    plt.xlabel('Inference Time (ms)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test images available for inference speed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5498d8",
   "metadata": {},
   "source": [
    "## 10. Comparative Analysis with YOLO Models\n",
    "\n",
    "Let's compare the RT-DETR-X performance with YOLO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d89db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for model comparison\n",
    "comparison_data = {\n",
    "    'Model': ['YOLOv8x', 'YOLOv11x', 'YOLOv12x', 'RT-DETR-R101', 'RT-DETR-X'],\n",
    "    'mAP50': [0.0, 0.0, 0.0, 0.0, 0.0],  # Placeholder values\n",
    "    'mAP50-95': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'Precision': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'Recall': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'Inference Time (ms/img)': [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "# Try to populate with actual values if available\n",
    "try:\n",
    "    # Current model results\n",
    "    comparison_data['mAP50'][4] = test_results.box.map50\n",
    "    comparison_data['mAP50-95'][4] = test_results.box.map\n",
    "    comparison_data['Precision'][4] = test_results.box.mp\n",
    "    comparison_data['Recall'][4] = test_results.box.mr\n",
    "    comparison_data['Inference Time (ms/img)'][4] = timing_results['mean_time']\n",
    "    \n",
    "    # Look for results from other models (this is just a placeholder)\n",
    "    other_result_dirs = [\n",
    "        \"../yolov8x_results/evaluation\",\n",
    "        \"../yolov11x_results/evaluation\", \n",
    "        \"../yolov12x_results/evaluation\",\n",
    "        \"../rtdetr_r101_results/evaluation\"\n",
    "    ]\n",
    "    \n",
    "    # In a real scenario, you would load the actual results from saved files\n",
    "    # This is just a placeholder with example values\n",
    "    example_values = {\n",
    "        'mAP50': [0.85, 0.86, 0.87, 0.84],\n",
    "        'mAP50-95': [0.65, 0.67, 0.68, 0.66],\n",
    "        'Precision': [0.82, 0.83, 0.84, 0.81],\n",
    "        'Recall': [0.80, 0.81, 0.82, 0.79],\n",
    "        'Inference Time (ms/img)': [12.5, 13.2, 13.8, 15.6]\n",
    "    }\n",
    "    \n",
    "    for i in range(4):\n",
    "        comparison_data['mAP50'][i] = example_values['mAP50'][i]\n",
    "        comparison_data['mAP50-95'][i] = example_values['mAP50-95'][i]\n",
    "        comparison_data['Precision'][i] = example_values['Precision'][i]\n",
    "        comparison_data['Recall'][i] = example_values['Recall'][i]\n",
    "        comparison_data['Inference Time (ms/img)'][i] = example_values['Inference Time (ms/img)'][i]\n",
    "    \n",
    "    # Create and display the comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Plot mAP comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Model', y='mAP50', data=comparison_df)\n",
    "    plt.title('mAP50 Comparison Across Models')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot inference time comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Model', y='Inference Time (ms/img)', data=comparison_df)\n",
    "    plt.title('Inference Time Comparison Across Models')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a scatter plot to visualize the trade-off between accuracy and speed\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='Inference Time (ms/img)', y='mAP50', data=comparison_df, s=100)\n",
    "    \n",
    "    # Add labels to each point\n",
    "    for i, model in enumerate(comparison_df['Model']):\n",
    "        plt.annotate(model, \n",
    "                    (comparison_df['Inference Time (ms/img)'][i], comparison_df['mAP50'][i]),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title('Accuracy vs. Speed Trade-off')\n",
    "    plt.xlabel('Inference Time (ms) - lower is better')\n",
    "    plt.ylabel('mAP50 - higher is better')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Couldn't create comparison: {e}\")\n",
    "    print(\"Run evaluations for all models to enable proper comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d01fca",
   "metadata": {},
   "source": [
    "## 11. Export Model for Deployment\n",
    "\n",
    "Let's save our fine-tuned model in different formats for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f662ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to different formats\n",
    "export_path = os.path.join(results_dir, \"exported_models\")\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "# Export to ONNX format\n",
    "model.export(format=\"onnx\", imgsz=640)\n",
    "\n",
    "print(f\"Models exported to {export_path}\")\n",
    "print(\"Available formats:\")\n",
    "for file in os.listdir(export_path):\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273a382",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusion\n",
    "\n",
    "We have successfully:\n",
    "1. Fine-tuned RT-DETR-X on a custom dataset\n",
    "2. Evaluated its performance on the test set\n",
    "3. Analyzed per-class metrics and visualized results\n",
    "4. Measured inference speed\n",
    "5. Compared performance with YOLO models\n",
    "6. Exported the model for deployment\n",
    "\n",
    "Key metrics:\n",
    "- mAP50: How accurate the model is at IoU threshold of 0.5\n",
    "- mAP50-95: How accurate the model is across multiple IoU thresholds\n",
    "- Precision: How many of the predicted detections are correct\n",
    "- Recall: How many of the ground truth objects are detected\n",
    "- Inference Speed: How fast the model processes images\n",
    "\n",
    "RT-DETR-X is the extra-large variant of the RT-DETR series, offering potentially higher accuracy at the cost of increased computational requirements. This model is suitable for applications where accuracy is more important than real-time performance, or when using more powerful hardware.\n",
    "\n",
    "The transformer-based architecture in RT-DETR provides improved feature extraction and modeling of relationships between objects in the image, which may lead to better performance in complex traffic scenes with multiple overlapping vehicles.\n",
    "\n",
    "To improve results further, consider:\n",
    "- Fine-tuning with a balanced class distribution\n",
    "- Using a smaller variant if faster inference is needed\n",
    "- Implementing TensorRT or other optimizations for deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
