{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e68dec",
   "metadata": {},
   "source": [
    "# YOLOv12x: Fine-tuning and Evaluation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Install required dependencies\n",
    "2. Prepare dataset for training\n",
    "3. Fine-tune YOLOv12x on a custom dataset\n",
    "4. Run inference on test images\n",
    "5. Calculate and visualize evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f5db5",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ultralytics package for YOLOv12\n",
    "!pip install ultralytics\n",
    "!pip install opencv-python matplotlib seaborn\n",
    "\n",
    "# Check CUDA availability \n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e7321",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61986b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a965ca9",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Let's assume we're working with a dataset that follows the YOLO format:\n",
    "- images/ folder containing training images\n",
    "- labels/ folder containing corresponding labels in YOLO format\n",
    "- A YAML configuration file describing classes and dataset paths\n",
    "\n",
    "If your dataset is structured differently, you'll need to adjust this section accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths - customize these for your specific project\n",
    "DATASET_DIR = \"../dataset_split\"  # Change this!\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "# # Create dataset configuration YAML\n",
    "# dataset_config = {\n",
    "#     'path': DATASET_DIR,\n",
    "#     'train': os.path.relpath(TRAIN_DIR, DATASET_DIR),\n",
    "#     'val': os.path.relpath(VAL_DIR, DATASET_DIR),\n",
    "#     'test': os.path.relpath(TEST_DIR, DATASET_DIR),\n",
    "#     'names': {\n",
    "#         # Add your class names and indices here\n",
    "#         # For example:\n",
    "#         # 0: 'car',\n",
    "#         # 1: 'truck',\n",
    "#         # 2: 'bus',\n",
    "#         # ...\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Write the dataset configuration to a YAML file\n",
    "yaml_path = os.path.join(DATASET_DIR, \"data.yaml\")\n",
    "# with open(yaml_path, 'w') as file:\n",
    "#     yaml.dump(dataset_config, file)\n",
    "\n",
    "# print(f\"Dataset configuration saved to: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3590d4fc",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning YOLOv12x Model\n",
    "\n",
    "Now we'll load a pre-trained YOLOv12x model and fine-tune it on our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b66cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLOv12x model\n",
    "model = YOLO('yolov12x.pt')\n",
    "\n",
    "# Define training hyperparameters optimized for small dataset (~400 images)\n",
    "hyperparameters = {\n",
    "    'epochs': 100,          # More epochs for small dataset\n",
    "    'batch': 8,             # Smaller batch size\n",
    "    'imgsz': 640,           # Image size\n",
    "    'patience': 20,         # Increased patience for early stopping\n",
    "    'device': 0,            # Device to use (0 for first GPU)\n",
    "    'workers': 4,           # Reduced worker threads\n",
    "    'optimizer': 'AdamW',   # Optimizer\n",
    "    'lr0': 0.0005,          # Lower initial learning rate\n",
    "    'lrf': 0.01,            # Final learning rate factor\n",
    "    'momentum': 0.937,      # SGD momentum\n",
    "    'weight_decay': 0.001,  # Increased weight decay to prevent overfitting\n",
    "    'warmup_epochs': 5.0,   # Longer warmup\n",
    "    'warmup_momentum': 0.8, # Warmup momentum\n",
    "    'warmup_bias_lr': 0.1,  # Warmup bias lr\n",
    "    'box': 7.5,             # Box loss gain\n",
    "    'cls': 0.5,             # Class loss gain\n",
    "    'hsv_h': 0.015,         # Image HSV-Hue augmentation\n",
    "    'hsv_s': 0.7,           # Image HSV-Saturation augmentation\n",
    "    'hsv_v': 0.4,           # Image HSV-Value augmentation\n",
    "    'translate': 0.2,       # Increased translation for better augmentation\n",
    "    'scale': 0.6,           # Increased scale variation\n",
    "    'fliplr': 0.5,          # Image flip left-right probability\n",
    "    'flipud': 0.2,          # Add up-down flipping\n",
    "    'mosaic': 1.0,          # Maximize mosaic augmentation\n",
    "    'mixup': 0.15,          # Add mixup augmentation\n",
    "    'copy_paste': 0.1,      # Add copy-paste augmentation\n",
    "}\n",
    "\n",
    "# Create model results directory\n",
    "results_dir = os.path.join(os.getcwd(), \"yolov12x_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    project=results_dir,\n",
    "    name='fine_tuned_model',\n",
    "    exist_ok=True,\n",
    "    **hyperparameters\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Model saved to: {os.path.join(results_dir, 'fine_tuned_model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001703f",
   "metadata": {},
   "source": [
    "## 5. Model Inference and Evaluation\n",
    "\n",
    "Now, let's evaluate the fine-tuned model on the test set and calculate performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model_path = os.path.join(results_dir, 'fine_tuned_model', 'weights', 'best.pt')\n",
    "model = YOLO(fine_tuned_model_path)\n",
    "\n",
    "# Run validation on the test set\n",
    "test_results = model.val(\n",
    "    data=yaml_path,\n",
    "    split='test',  # Use the test split\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    verbose=True,\n",
    "    conf=0.25,    # Confidence threshold\n",
    "    iou=0.5,      # IoU threshold\n",
    "    project=results_dir,\n",
    "    name='evaluation',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(\"Test results summary:\")\n",
    "print(f\"mAP50: {test_results.box.map50:.5f}\")\n",
    "print(f\"mAP50-95: {test_results.box.map:.5f}\")\n",
    "print(f\"Precision: {test_results.box.mp:.5f}\")\n",
    "print(f\"Recall: {test_results.box.mr:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3e7eea",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis per Class\n",
    "\n",
    "Let's analyze the model performance for each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class-wise metrics from the validation results\n",
    "class_map = test_results.names  # Class index to name mapping\n",
    "class_metrics = test_results.metrics.cls  # Class metrics\n",
    "class_precisions = test_results.metrics.precision  # Class precisions\n",
    "class_recalls = test_results.metrics.recall  # Class recalls\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': [class_map[i] for i in range(len(class_map))],\n",
    "    'AP50': test_results.ap50_per_class,\n",
    "    'AP50-95': test_results.ap_per_class,\n",
    "    'Precision': class_precisions,\n",
    "    'Recall': class_recalls\n",
    "})\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Plot AP50 for each class\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Class', y='AP50', data=metrics_df)\n",
    "plt.title('AP50 for Each Class')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8f628",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix\n",
    "\n",
    "The confusion matrix helps us see how well the model differentiates between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_matrix = test_results.confusion_matrix.matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    conf_matrix / np.sum(conf_matrix, axis=1)[:, None],  # Normalize by row (true classes)\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=list(class_map.values()),\n",
    "    yticklabels=list(class_map.values())\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6fb78",
   "metadata": {},
   "source": [
    "## 8. Visualizing Detection Results on Test Images\n",
    "\n",
    "Let's visualize some predictions on test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of test images\n",
    "test_images_dir = os.path.join(TEST_DIR, 'images')\n",
    "test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "test_images = [str(img) for img in test_images]\n",
    "\n",
    "# Select random images for visualization\n",
    "if len(test_images) > 0:\n",
    "    sample_images = random.sample(test_images, min(5, len(test_images)))\n",
    "    \n",
    "    for img_path in sample_images:\n",
    "        # Run inference\n",
    "        results = model(img_path, conf=0.25)\n",
    "        \n",
    "        # Display results\n",
    "        for result in results:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
    "            img = result.orig_img\n",
    "            \n",
    "            # Plot detections\n",
    "            for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
    "                x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "                class_id = int(cls.item())\n",
    "                class_name = class_map[class_id]\n",
    "                confidence = conf.item()\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{class_name}: {confidence:.2f}\"\n",
    "                cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f\"Predictions on {os.path.basename(img_path)}\")\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No test images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d3f8e",
   "metadata": {},
   "source": [
    "## 9. Precision-Recall Curves\n",
    "\n",
    "Let's plot precision-recall curves for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create P-R curve plots for each class\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get P-R curve data from results\n",
    "precision_data = test_results.curves[0].data\n",
    "recall_data = test_results.curves[1].data\n",
    "\n",
    "# Plot P-R curves for each class\n",
    "for i in range(len(class_map)):\n",
    "    plt.plot(recall_data[:, i], precision_data[:, i], label=f'{class_map[i]}')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a799e",
   "metadata": {},
   "source": [
    "## 10. Export Model for Deployment\n",
    "\n",
    "Let's save our fine-tuned model in different formats for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63987f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to different formats\n",
    "export_path = os.path.join(results_dir, \"exported_models\")\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "# Export to ONNX format\n",
    "model.export(format=\"onnx\", imgsz=640)\n",
    "\n",
    "# Export to TorchScript format\n",
    "model.export(format=\"torchscript\", imgsz=640)\n",
    "\n",
    "print(f\"Models exported to {export_path}\")\n",
    "print(\"Available formats:\")\n",
    "for file in os.listdir(export_path):\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06811446",
   "metadata": {},
   "source": [
    "## 11. Comparative Analysis with Previous YOLO Versions\n",
    "\n",
    "Let's compare YOLOv12x with previous YOLO versions to highlight improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c076372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance metrics from other models (if available)\n",
    "comparison_metrics = {\n",
    "    'Model': ['YOLOv8x', 'YOLOv11x', 'YOLOv12x'],\n",
    "    'mAP50': [0.0, 0.0, 0.0],  # Placeholder values\n",
    "    'mAP50-95': [0.0, 0.0, 0.0],\n",
    "    'Precision': [0.0, 0.0, 0.0],\n",
    "    'Recall': [0.0, 0.0, 0.0],\n",
    "    'Inference Time (ms/img)': [0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "# Try to load actual metrics if available\n",
    "try:\n",
    "    # YOLOv8 metrics\n",
    "    yolov8_results_path = \"../yolov8x_results/evaluation\"\n",
    "    if os.path.exists(yolov8_results_path):\n",
    "        # Get the metrics (this is just a placeholder, actual implementation depends on how metrics are stored)\n",
    "        comparison_metrics['mAP50'][0] = 0.85  # Example value\n",
    "        comparison_metrics['mAP50-95'][0] = 0.65  # Example value\n",
    "        comparison_metrics['Precision'][0] = 0.82  # Example value\n",
    "        comparison_metrics['Recall'][0] = 0.80  # Example value\n",
    "        comparison_metrics['Inference Time (ms/img)'][0] = 12.5  # Example value\n",
    "    \n",
    "    # YOLOv11 metrics\n",
    "    yolov11_results_path = \"../yolov11x_results/evaluation\"\n",
    "    if os.path.exists(yolov11_results_path):\n",
    "        # Get the metrics (this is just a placeholder, actual implementation depends on how metrics are stored)\n",
    "        comparison_metrics['mAP50'][1] = 0.87  # Example value\n",
    "        comparison_metrics['mAP50-95'][1] = 0.67  # Example value\n",
    "        comparison_metrics['Precision'][1] = 0.84  # Example value\n",
    "        comparison_metrics['Recall'][1] = 0.81  # Example value\n",
    "        comparison_metrics['Inference Time (ms/img)'][1] = 11.8  # Example value\n",
    "    \n",
    "    # Current YOLOv12 metrics\n",
    "    comparison_metrics['mAP50'][2] = test_results.box.map50\n",
    "    comparison_metrics['mAP50-95'][2] = test_results.box.map\n",
    "    comparison_metrics['Precision'][2] = test_results.box.mp\n",
    "    comparison_metrics['Recall'][2] = test_results.box.mr\n",
    "    # You'd need to calculate inference time separately\n",
    "    comparison_metrics['Inference Time (ms/img)'][2] = 11.0  # Example value\n",
    "    \n",
    "    # Create DataFrame and visualize comparison\n",
    "    comparison_df = pd.DataFrame(comparison_metrics)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Plot comparison metrics\n",
    "    metrics_to_plot = ['mAP50', 'mAP50-95', 'Precision', 'Recall']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        sns.barplot(x='Model', y=metric, data=comparison_df)\n",
    "        plt.title(f'Comparison of {metric}')\n",
    "        plt.ylabel(metric)\n",
    "        plt.ylim([0, 1])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot inference time comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Model', y='Inference Time (ms/img)', data=comparison_df)\n",
    "    plt.title('Inference Speed Comparison')\n",
    "    plt.ylabel('Time (ms/image)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Couldn't load comparison metrics: {e}\")\n",
    "    print(\"Run all model evaluations first to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b494037",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusion\n",
    "\n",
    "We have successfully:\n",
    "1. Fine-tuned YOLOv12x on a custom dataset\n",
    "2. Evaluated its performance on the test set\n",
    "3. Analyzed per-class metrics and visualized results\n",
    "4. Exported the model for deployment\n",
    "5. Compared performance with earlier YOLO versions\n",
    "\n",
    "Key metrics:\n",
    "- mAP50: How accurate the model is at IoU threshold of 0.5\n",
    "- mAP50-95: How accurate the model is across multiple IoU thresholds\n",
    "- Precision: How many of the predicted detections are correct\n",
    "- Recall: How many of the ground truth objects are detected\n",
    "\n",
    "Improvements in YOLOv12x compared to earlier versions:\n",
    "- [List key improvements based on your comparative analysis]\n",
    "\n",
    "To improve results further, consider:\n",
    "- Increasing the number of training epochs\n",
    "- Adding more training data or using data augmentation\n",
    "- Adjusting hyperparameters like learning rate and batch size\n",
    "- Using different model variants (YOLOv12s, YOLOv12m, etc.)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
