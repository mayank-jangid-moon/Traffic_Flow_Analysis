{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f65692d",
   "metadata": {},
   "source": [
    "# YOLOv8x: Fine-tuning and Evaluation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Install required dependencies\n",
    "2. Prepare dataset for training\n",
    "3. Fine-tune YOLOv8x on a custom dataset\n",
    "4. Run inference on test images\n",
    "5. Calculate and visualize evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eef14c",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea9220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu126\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install ultralytics package for YOLOv8\n",
    "!pip install ultralytics\n",
    "!pip install opencv-python matplotlib seaborn\n",
    "\n",
    "# Check CUDA availability \n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3036785",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d57bb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bce5a",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Let's assume we're working with a dataset that follows the YOLO format:\n",
    "- images/ folder containing training images\n",
    "- labels/ folder containing corresponding labels in YOLO format\n",
    "- A YAML configuration file describing classes and dataset paths\n",
    "\n",
    "If your dataset is structured differently, you'll need to adjust this section accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db88765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths - customize these for your specific project\n",
    "DATASET_DIR = \"../dataset_split\"  # Change this!\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test\")\n",
    "\n",
    "# # Create dataset configuration YAML\n",
    "# dataset_config = {\n",
    "#     'path': DATASET_DIR,\n",
    "#     'train': os.path.relpath(TRAIN_DIR, DATASET_DIR),\n",
    "#     'val': os.path.relpath(VAL_DIR, DATASET_DIR),\n",
    "#     'test': os.path.relpath(TEST_DIR, DATASET_DIR),\n",
    "#     'names': {\n",
    "#         # Add your class names and indices here\n",
    "#         # For example:\n",
    "#         # 0: 'car',\n",
    "#         # 1: 'truck',\n",
    "#         # 2: 'bus',\n",
    "#         # ...\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# Write the dataset configuration to a YAML file\n",
    "yaml_path = os.path.join(DATASET_DIR, \"data.yaml\")\n",
    "# with open(yaml_path, 'w') as file:\n",
    "#     yaml.dump(dataset_config, file)\n",
    "\n",
    "# print(f\"Dataset configuration saved to: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20dbe1",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning YOLOv8x Model\n",
    "\n",
    "Now we'll load a pre-trained YOLOv8x model and fine-tune it on our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.155 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.153 ðŸš€ Python-3.12.3 torch-2.7.0+cu126 CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../dataset_split/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8x.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fine_tuned_model, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/comparative_study/yolov8x_results, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/comparative_study/yolov8x_results/fine_tuned_model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/mayank/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 2.21MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      "  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n",
      " 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n",
      " 22        [15, 18, 21]  1   8718931  ultralytics.nn.modules.head.Detect           [1, [320, 640, 640]]          \n",
      "Model summary: 209 layers, 68,153,571 parameters, 68,153,555 gradients, 258.1 GFLOPs\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 132.8Â±31.5 MB/s, size: 59.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/dataset_split/train/labels... 438 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 2525.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/dataset_split/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 167.8Â±47.9 MB/s, size: 60.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/mayank/.globalenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/dataset_split/val/labels... 93 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:00<00:00, 2255.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/dataset_split/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/mayank/.globalenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/comparative_study/yolov8x_results/fine_tuned_model/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/mayank/Vault/work_space/AIMS/Summer Project/Traffic_Flow_Analysis/comparative_study/yolov8x_results/fine_tuned_model\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load pre-trained YOLOv8x model\n",
    "model = YOLO('yolov8x.pt')\n",
    "\n",
    "# Define training hyperparameters optimized for small dataset (~400 images)\n",
    "hyperparameters = {\n",
    "    'epochs': 100,          # More epochs for small dataset\n",
    "    'batch': 8,             # Smaller batch size\n",
    "    'imgsz': 640,           # Image size\n",
    "    'patience': 20,         # Increased patience for early stopping\n",
    "    'device': 0,            # Device to use (0 for first GPU)\n",
    "    'workers': 4,           # Reduced worker threads\n",
    "    'optimizer': 'AdamW',   # Optimizer\n",
    "    'lr0': 0.0005,          # Lower initial learning rate\n",
    "    'lrf': 0.01,            # Final learning rate factor\n",
    "    'momentum': 0.937,      # SGD momentum\n",
    "    'weight_decay': 0.001,  # Increased weight decay to prevent overfitting\n",
    "    'warmup_epochs': 5.0,   # Longer warmup\n",
    "    'warmup_momentum': 0.8, # Warmup momentum\n",
    "    'warmup_bias_lr': 0.1,  # Warmup bias lr\n",
    "    'box': 7.5,             # Box loss gain\n",
    "    'cls': 0.5,             # Class loss gain\n",
    "    'hsv_h': 0.015,         # Image HSV-Hue augmentation\n",
    "    'hsv_s': 0.7,           # Image HSV-Saturation augmentation\n",
    "    'hsv_v': 0.4,           # Image HSV-Value augmentation\n",
    "    'translate': 0.2,       # Increased translation for better augmentation\n",
    "    'scale': 0.6,           # Increased scale variation\n",
    "    'fliplr': 0.5,          # Image flip left-right probability\n",
    "    'flipud': 0.2,          # Add up-down flipping\n",
    "    'mosaic': 1.0,          # Maximize mosaic augmentation\n",
    "    'mixup': 0.15,          # Add mixup augmentation\n",
    "    'copy_paste': 0.1,      # Add copy-paste augmentation\n",
    "}\n",
    "\n",
    "# Create model results directory\n",
    "results_dir = os.path.join(os.getcwd(), \"yolov8x_results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    project=results_dir,\n",
    "    name='fine_tuned_model',\n",
    "    exist_ok=True,\n",
    "    **hyperparameters\n",
    ")\n",
    "\n",
    "print(f\"Training completed. Model saved to: {os.path.join(results_dir, 'fine_tuned_model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41836a5e",
   "metadata": {},
   "source": [
    "## 5. Model Inference and Evaluation\n",
    "\n",
    "Now, let's evaluate the fine-tuned model on the test set and calculate performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model_path = os.path.join(results_dir, 'fine_tuned_model', 'weights', 'best.pt')\n",
    "model = YOLO(fine_tuned_model_path)\n",
    "\n",
    "# Run validation on the test set\n",
    "test_results = model.val(\n",
    "    data=yaml_path,\n",
    "    split='test',  # Use the test split\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    verbose=True,\n",
    "    conf=0.25,    # Confidence threshold\n",
    "    iou=0.5,      # IoU threshold\n",
    "    project=results_dir,\n",
    "    name='evaluation',\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "print(\"Test results summary:\")\n",
    "print(f\"mAP50: {test_results.box.map50:.5f}\")\n",
    "print(f\"mAP50-95: {test_results.box.map:.5f}\")\n",
    "print(f\"Precision: {test_results.box.mp:.5f}\")\n",
    "print(f\"Recall: {test_results.box.mr:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424d57e",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis per Class\n",
    "\n",
    "Let's analyze the model performance for each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ca8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class-wise metrics from the validation results\n",
    "class_map = test_results.names  # Class index to name mapping\n",
    "\n",
    "# Access class metrics correctly from test_results.box\n",
    "# The DetMetrics object doesn't have a 'metrics' attribute as per the error\n",
    "class_precisions = test_results.box.p  # Class precisions\n",
    "class_recalls = test_results.box.r     # Class recalls\n",
    "ap50_per_class = test_results.box.ap50  # AP50 per class\n",
    "ap_per_class = test_results.box.ap      # AP50-95 per class\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': [class_map[i] for i in range(len(class_map))],\n",
    "    'AP50': ap50_per_class,\n",
    "    'AP50-95': ap_per_class,\n",
    "    'Precision': class_precisions,\n",
    "    'Recall': class_recalls\n",
    "})\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Plot AP50 for each class\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Class', y='AP50', data=metrics_df)\n",
    "plt.title('AP50 for Each Class')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033d0ad4",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix\n",
    "\n",
    "The confusion matrix helps us see how well the model differentiates between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "conf_matrix = test_results.confusion_matrix.matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    conf_matrix / np.sum(conf_matrix, axis=1)[:, None],  # Normalize by row (true classes)\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=list(class_map.values()),\n",
    "    yticklabels=list(class_map.values())\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe8870",
   "metadata": {},
   "source": [
    "## 8. Visualizing Detection Results on Test Images\n",
    "\n",
    "Let's visualize some predictions on test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d144cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of test images\n",
    "test_images_dir = os.path.join(TEST_DIR, 'images')\n",
    "test_images = list(Path(test_images_dir).glob('*.jpg')) + list(Path(test_images_dir).glob('*.png'))\n",
    "test_images = [str(img) for img in test_images]\n",
    "\n",
    "# Select random images for visualization\n",
    "if len(test_images) > 0:\n",
    "    sample_images = random.sample(test_images, min(5, len(test_images)))\n",
    "    \n",
    "    for img_path in sample_images:\n",
    "        # Run inference\n",
    "        results = model(img_path, conf=0.25)\n",
    "        \n",
    "        # Display results\n",
    "        for result in results:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 9))\n",
    "            img = result.orig_img\n",
    "            \n",
    "            # Plot detections\n",
    "            for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
    "                x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "                class_id = int(cls.item())\n",
    "                class_name = class_map[class_id]\n",
    "                confidence = conf.item()\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{class_name}: {confidence:.2f}\"\n",
    "                cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            ax.set_title(f\"Predictions on {os.path.basename(img_path)}\")\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No test images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08580e",
   "metadata": {},
   "source": [
    "## 9. Precision-Recall Curves\n",
    "\n",
    "Let's plot precision-recall curves for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cac335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create P-R curve plots for each class\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get P-R curve data from results\n",
    "# Make sure to access the curves data correctly\n",
    "try:\n",
    "    # First attempt - if curves are stored directly in test_results\n",
    "    precision_data = test_results.curves[0].data\n",
    "    recall_data = test_results.curves[1].data\n",
    "except (AttributeError, IndexError):\n",
    "    # Alternative access method - check if it's in box\n",
    "    try:\n",
    "        precision_data = test_results.box.curves[0].data\n",
    "        recall_data = test_results.box.curves[1].data\n",
    "    except (AttributeError, IndexError):\n",
    "        # If can't access curves, create simple PR curve from class values\n",
    "        print(\"Could not access PR curves directly, creating simplified version from class values\")\n",
    "        precision_data = np.array([class_precisions]).T\n",
    "        recall_data = np.array([class_recalls]).T\n",
    "\n",
    "# Plot P-R curves for each class\n",
    "for i in range(len(class_map)):\n",
    "    try:\n",
    "        plt.plot(recall_data[:, i], precision_data[:, i], label=f'{class_map[i]}')\n",
    "    except IndexError:\n",
    "        plt.scatter(class_recalls[i], class_precisions[i], label=f'{class_map[i]}')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2daf73c",
   "metadata": {},
   "source": [
    "## 10. Export Model for Deployment\n",
    "\n",
    "Let's save our fine-tuned model in different formats for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to different formats\n",
    "export_path = os.path.join(results_dir, \"exported_models\")\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "# Export to ONNX format\n",
    "model.export(format=\"onnx\", imgsz=640)\n",
    "\n",
    "# Export to TorchScript format\n",
    "model.export(format=\"torchscript\", imgsz=640)\n",
    "\n",
    "print(f\"Models exported to {export_path}\")\n",
    "print(\"Available formats:\")\n",
    "for file in os.listdir(export_path):\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c2923",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusion\n",
    "\n",
    "We have successfully:\n",
    "1. Fine-tuned YOLOv8x on a custom dataset\n",
    "2. Evaluated its performance on the test set\n",
    "3. Analyzed per-class metrics and visualized results\n",
    "4. Exported the model for deployment\n",
    "\n",
    "Key metrics:\n",
    "- mAP50: How accurate the model is at IoU threshold of 0.5\n",
    "- mAP50-95: How accurate the model is across multiple IoU thresholds\n",
    "- Precision: How many of the predicted detections are correct\n",
    "- Recall: How many of the ground truth objects are detected\n",
    "\n",
    "To improve results further, consider:\n",
    "- Increasing the number of training epochs\n",
    "- Adding more training data or using data augmentation\n",
    "- Adjusting hyperparameters like learning rate and batch size\n",
    "- Using different model variants (YOLOv8s, YOLOv8m, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".globalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
